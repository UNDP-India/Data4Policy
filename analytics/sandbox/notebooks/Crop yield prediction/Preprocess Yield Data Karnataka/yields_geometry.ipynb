{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from fuzzywuzzy import process\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a shapeName column containing the names of each Insurance Unit in lowercase and stripped\n",
    "insurance_names = pd.read_excel(r'C:\\Users\\mieke\\Documents\\Msc Thesis\\Datasets\\Yield data\\Karnataka\\insurance_names.xlsx', index_col = 0)\n",
    "insurance_names['shapeName'] = insurance_names['Insurance Unit'].copy()\n",
    "insurance_names.shapeName = insurance_names.shapeName.str.lower()\n",
    "insurance_names.shapeName = insurance_names.shapeName.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import files related to yields and insurance units\n",
    "yields = pd.read_excel(r'C:\\Users\\mieke\\Documents\\Msc Thesis\\Datasets\\Yield data\\Karnataka\\yields.xlsx', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound_types = ['gp', 'h'] # Different bound types\n",
    "column_names = ['KGISGPName', 'KGISHobliN'] # Column names containing the name of the Insurance Unit\n",
    "path_types = ['C:/Users/mieke/Documents/Msc Thesis/Datasets/Shapefiles/Karnataka_gp_shp/', 'C:/Users/mieke/Documents/Msc Thesis/Datasets/Shapefiles/Karnataka_h_shp/'] # Different paths\n",
    "non_indices = [[28], [28, 10]] # District names which do not occur in the yields dataframe (descending order is important) (eg 28:vijayanagara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell combines insurance unit names as given in the yields data with insurance unit names as given in the boundary data for hobli as well as gram panchayat\n",
    "\n",
    "# Create empty dataframes\n",
    "df_yields_geometry = pd.DataFrame() \n",
    "df_geometry = pd.DataFrame()\n",
    "\n",
    "# Loop over each of the bound types separately\n",
    "for t in range(len(bound_types)):\n",
    "    df_part_names = insurance_names[insurance_names['Gram Panchayat/Hobli'] == bound_types[t]] # create df including the rows within insurance names df belonging to the current bound type \n",
    "    df_part_names = df_part_names[['shapeName', 'Insurance Unit', 'District']].copy() # select the relevant columns\n",
    "    df_part_names = df_part_names[['shapeName', 'Insurance Unit', 'District']].drop_duplicates(ignore_index=True) # drop duplicates\n",
    "\n",
    "    part_names = df_part_names['shapeName'].unique() # list of all unique shapenames within the dataframe with current bound type\n",
    "    part_names = sorted(part_names) # sort the shapenames in ascending order\n",
    "\n",
    "    path = path_types[t] # Set path\n",
    "    folder_names = os.listdir(path) # contains all foldernames within the folder\n",
    "\n",
    "    district_names = [] # create list to include all unique district names within the bounds geodatframe\n",
    "    df_total = pd.DataFrame() # create empty dataframe\n",
    "    # This for loop creates one big geodataframe out of all separate district wise geodataframes\n",
    "    for i in folder_names:\n",
    "        input_shp = gpd.read_file(path + i + '/' + i + '.shp')\n",
    "        district_name = i[3:].lower() # district name as used within the bounds geodataframe\n",
    "        district_names.append(district_name)\n",
    "        input_shp['District'] = district_name\n",
    "        df_total = pd.concat([df_total, input_shp], ignore_index=True)\n",
    "\n",
    "    district_names = sorted(district_names) # sort district names in ascending order\n",
    "    for k in non_indices[t]:\n",
    "        district_names.pop(k) # drop all district names which are not in the yields dataframe for the current bound type\n",
    "\n",
    "    # select the relevant columns of the geometry bounds geodataframe and sort them based on their Insurance Unit name\n",
    "    insurance_bounds = df_total[[column_names[t], 'District', 'geometry']].sort_values([column_names[t], 'District'], ascending=True, ignore_index=True)\n",
    "    # Create a shapeName column containing the names of each bound in lowercase and stripped\n",
    "    insurance_bounds['shapeName'] = insurance_bounds[column_names[t]]\n",
    "    insurance_bounds['shapeName'] = insurance_bounds.shapeName.str.lower()\n",
    "    insurance_bounds['shapeName'] = insurance_bounds.shapeName.str.strip()\n",
    "\n",
    "    df_part_geometry = insurance_bounds[['shapeName', column_names[t], 'District', 'geometry']].copy() # copy the previously created geodataframe\n",
    "    df_part_geometry = df_part_geometry.drop_duplicates(subset = ['shapeName', column_names[t], 'District'], ignore_index=True) # drop duplicates if they have the same bound name and district\n",
    "    df_part_geometry = df_part_geometry.sort_values(['shapeName'], ascending=True, ignore_index=True) # sort the geodataframe based on shapeName\n",
    "    part_bounds = df_part_geometry['shapeName'].unique() # list of all unique shapenames within the bounds geodataframe with current bound type\n",
    "\n",
    "    unique_districts = df_part_names['District'].unique() # list of all unique district names within the yields dataframe\n",
    "    unique_districts = sorted(unique_districts) # sort the district names in ascending order\n",
    "\n",
    "    # Make sure that the district names within the yields dataframe are equal to the spelling of the district names within the bounds geodataframe\n",
    "    for i in range(len(unique_districts)):\n",
    "        df_part_names['District'] = np.where(df_part_names['District'] == unique_districts[i], district_names[i], df_part_names['District'])\n",
    "\n",
    "    # Create new columns\n",
    "    df_part_names['Insurance_Bound'] = np.nan # \n",
    "    df_part_names['Insurance_Bound_f'] = np.nan #\n",
    "    df_part_names['Similarity'] = np.nan # Highest similarity score\n",
    "    df_part_names['geometry'] = np.nan # geometry\n",
    "\n",
    "    # This loop finds the shapeName within the bounds geodataframe which is most similar to the shapeName within the yields dataframe\n",
    "    for d in district_names:\n",
    "        df_bounds = df_part_geometry[df_part_geometry['District'] == d].copy() # contains all rows for district d (bounds file)\n",
    "        df_names = df_part_names[df_part_names['District'] == d].copy() # contains all rows for district d (yields file)\n",
    "        # For every shapeName within the yields file, we check which shapeName within the bounds file is most similar to it\n",
    "        # Note that it only considers the names within the same district, since sometimes a name is used multiple times\n",
    "        for i in df_names.index:\n",
    "            indices = []\n",
    "            for j in df_bounds.index:\n",
    "                indices.append(fuzz.token_set_ratio(df_names.loc[i, 'shapeName'],df_bounds.loc[j, 'shapeName'])) # in this case, better than sort_ratio\n",
    "            df_part_names.loc[i,'Insurance_Bound'] = df_bounds.loc[df_bounds.index[np.argmax(indices)],column_names[t]] # the original Insurance Name corresponding to the highest similarity bound\n",
    "            df_part_names.loc[i,'Insurance_Bound_f'] = df_bounds.loc[df_bounds.index[np.argmax(indices)],'shapeName'] # the shapeName corresponding to the highest similarity bound\n",
    "            df_part_names.loc[i,'Similarity'] = np.max(indices) # the highest similarity found for the current insurance name\n",
    "            df_part_names.loc[i, 'geometry'] = df_bounds.loc[df_bounds.index[np.argmax(indices)],'geometry'] # geometry corresponding to the highest similarity bound\n",
    "\n",
    "    df_final = df_part_names[df_part_names['Similarity'] == 100] # only keep the Insurance Units for which we have derived a 100% similarity\n",
    "\n",
    "    ## YIELDS PART\n",
    "    yields_part = yields[yields['Gram Panchayat/Hobli'] == bound_types[t]] # create df including the rows within yields df belonging to the current bound type \n",
    "\n",
    "    unique_districts = yields_part['District'].unique() # list of all unique district names within the yields dataframe\n",
    "    unique_districts = sorted(unique_districts) # sort the district names in ascending order\n",
    "\n",
    "    # Make sure that the district names within the yields dataframe are equal to the spelling of the district names within the bounds geodataframe\n",
    "    for i in range(len(unique_districts)):\n",
    "        yields_part['District'] = np.where(yields_part['District'] == unique_districts[i], district_names[i], yields_part['District'])\n",
    "\n",
    "    # Merge the yields datafrme with created geometry bounds geodataframe for current bound type, based on Insurance Unit and District \n",
    "    merged_df = yields_part.merge(df_part_names, on=['Insurance Unit', 'District'])#, how='left')\n",
    "    # mergedDf_drop = mergedDf.dropna(subset='geometry') # Note: note needed, as it is immediately done. It is needed, when we use how='left' above\n",
    "\n",
    "    df_part = merged_df[['Year', 'Season', 'Insurance Unit', 'Gram Panchayat/Hobli', 'District', 'Taluk', 'Crop', 'IRR_RF', 'Average Yield(Kg/Ha)', 'geometry']].copy() # select the relevant columns\n",
    "    # df_part['Gram Panchayat/Hobli'] = bound_types[t] # Note: note needed, as it is already added above \n",
    "\n",
    "    df_yields_geometry = pd.concat([df_yields_geometry, df_part], ignore_index=True) # add the yields_geometry results for the current bound type to the final dataframe\n",
    "\n",
    "    df_part_names['Gram Panchayat/Hobli'] = bound_types[t] # Add current bound type to dataframe \n",
    "    df_geometry = pd.concat([df_geometry, df_part_names], ignore_index=True) # add the geometry results for the current bound type to the final dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mieke\\anaconda3\\envs\\thesis_base\\lib\\site-packages\\pyproj\\crs\\crs.py:130: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "c:\\Users\\mieke\\anaconda3\\envs\\thesis_base\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n",
      "c:\\Users\\mieke\\anaconda3\\envs\\thesis_base\\lib\\site-packages\\pyproj\\crs\\crs.py:130: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "c:\\Users\\mieke\\anaconda3\\envs\\thesis_base\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Transform the geometry and yields_geometry dataframes to geodataframes\n",
    "# Then, save the geometry and yields_geometry geodataframes\n",
    "gdf_yields_geometry = gpd.GeoDataFrame(df_yields_geometry, geometry = df_yields_geometry.geometry, crs = {'init': 'epsg:32643'}) # 32643 is the original crs (found by using .crs for one of the input shp files)\n",
    "gdf_yields_geometry.geometry = gdf_yields_geometry.geometry.to_crs(epsg = 4326) # Set crs to 4326\n",
    "gdf_yields_geometry.to_file(r'C:\\Users\\mieke\\Documents\\Msc Thesis\\Datasets\\Yield Data\\Karnataka\\yields_geometry.geojson', driver=\"GeoJSON\") \n",
    "\n",
    "gdf_geometry = gpd.GeoDataFrame(df_geometry, geometry = df_geometry.geometry, crs = {'init': 'epsg:32643'}) # 32643 is the original crs (found by using .crs for one of the input shp files)\n",
    "gdf_geometry.geometry = gdf_geometry.geometry.to_crs(epsg = 4326)\n",
    "gdf_geometry.to_file(r'C:\\Users\\mieke\\Documents\\Msc Thesis\\Datasets\\Yield Data\\Karnataka\\geometry.geojson', driver=\"GeoJSON\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('thesis_base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "734205a0c94b5e97d2e885f269ea6bd400da4ba9a6c87b08a93b97fdaab1acc8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
