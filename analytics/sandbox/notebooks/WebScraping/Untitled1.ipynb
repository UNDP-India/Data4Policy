{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import newspaper\n",
    "import geopandas as gpd\n",
    "\n",
    "from GoogleNews import GoogleNews\n",
    "googlenews = GoogleNews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read geodataframe for mandal and district names to search in news articles\n",
    "gdf = gpd.read_file('../DPPD/data/TSDM/Mandal_Boundary.shp')\n",
    "mandal = list(gdf.Mandal_Nam.unique()) #unique mandals\n",
    "dist = list(gdf.Dist_Name.unique())    #unique districts\n",
    "dist = dist[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'group'\n"
     ]
    }
   ],
   "source": [
    "googlenews = GoogleNews(start='01/01/2022',end='07/12/2022')\n",
    "df_reclist =[]\n",
    "news1list = []\n",
    "for distname in dist:\n",
    "    try:\n",
    "        \n",
    "        googlenews.search('agriculture '+distname)\n",
    "        for i in range(2,20):\n",
    "            googlenews.getpage(i)\n",
    "            df_records = pd.DataFrame(googlenews.results())#[['title','desc','link']]\n",
    "        df_reclist.append(df_records)\n",
    "        #Scrape the list of article dictionary from news list\n",
    "        news_articles = []\n",
    "        for i in df_records.link:\n",
    "\n",
    "            article = newspaper.Article(url=i, language='en')\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            article ={\n",
    "                \"title\": str(article.title),\n",
    "                \"text\": str(article.text),\n",
    "                \"authors\": article.authors,\n",
    "                \"published_date\": str(article.publish_date),\n",
    "                \"top_image\": str(article.top_image),\n",
    "                \"videos\": article.movies,\n",
    "                \"keywords\": article.keywords,\n",
    "                \"summary\": str(article.summary)\n",
    "            }\n",
    "            news_articles.append(article)\n",
    "            \n",
    "            #Convert list of news articles into dataframe with all scraped information\n",
    "            news = pd.DataFrame(news_articles)\n",
    "\n",
    "            # Access list of mandal mentione in each article\n",
    "\n",
    "            keyword_mandal = []\n",
    "            mandal_list = []\n",
    "\n",
    "            for text in  news.text:\n",
    "                for i in range(0, len(mandal)):\n",
    "                    if mandal[i] in text:\n",
    "                        keyword_mandal.append(mandal[i])\n",
    "                # insert the list to the set\n",
    "                    list_set = set(keyword_mandal)\n",
    "                # convert the set to the list\n",
    "                    unique_list = (list(list_set))\n",
    "                    unique_list = ','.join(str(item) for item in unique_list)\n",
    "                mandal_list.append(unique_list)\n",
    "\n",
    "                # Access list of district mentione in each article\n",
    "            keyword_dist = []\n",
    "            dist_list = []\n",
    "            for text in  news.text:\n",
    "                for i in range(0, len(dist)):\n",
    "                    if dist[i] in text:\n",
    "                        keyword_dist.append(dist[i])\n",
    "                # insert the list to the set\n",
    "                    list_set = set(keyword_dist)\n",
    "                # convert the set to the list\n",
    "                    unique_list = (list(list_set))\n",
    "                    unique_list = ','.join(str(item) for item in unique_list)\n",
    "                dist_list.append(unique_list)\n",
    "\n",
    "                #add mandal and districts to news dataframe\n",
    "        news['Mandals'] = mandal_list\n",
    "        news['Districts'] = dist_list\n",
    "\n",
    "        #convert full news dataframe to csv\n",
    "        #news.to_csv('news_full.csv', index = False)\n",
    "\n",
    "        #Dataframe with few columns\n",
    "        news1 = news[['title','Mandals','Districts','published_date']]\n",
    "        #news1.to_csv('news.csv', index = False)\n",
    "        news1list.append(news1)\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/luckyw0w/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from GoogleNews import GoogleNews\n",
    "from newspaper import Article\n",
    "from newspaper import Config\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "nltk.download('vader_lexicon') #required for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name = input(\"Please provide the name of the Company or a Ticker: \")\n",
    "#As long as the company name is valid, not empty...\n",
    "if company_name != '':\n",
    "    print(f'Searching for and analyzing {company_name}, Please be patient, it might take a while...')\n",
    "\n",
    "    #Extract News with Google News\n",
    "    googlenews = GoogleNews(start=yesterday,end=now)\n",
    "    googlenews.search(company_name)\n",
    "    result = googlenews.result()\n",
    "    #store the results\n",
    "    df = pd.DataFrame(result)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'group'\n"
     ]
    }
   ],
   "source": [
    "googlenews = GoogleNews()\n",
    "googlenews.headers = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Cafari/537.36'\n",
    "googlenews = GoogleNews(start='01/12/2022',end='07/12/2022')\n",
    "googlenews.search('agriculture Khammam')\n",
    "for i in range(1,20):\n",
    "    googlenews.getpage(i) \n",
    "result=googlenews.result()\n",
    "df=pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "googlenews = GoogleNews()\n",
    "#googlenews.headers = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Safari/537.36'\n",
    "googlenews = GoogleNews(start='01/12/2022',end='07/12/2022')\n",
    "googlenews.search('agriculture Khammam')\n",
    "len(googlenews.get_links())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenews.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
